<!DOCTYPE HTML>

<html>
	<head>
		<title>Funcionamiento</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

        		<!-- Wrapper -->
			<div id="wrapper" class="fade-in">

				<!-- Intro -->
					<header id="header">
						<a href="../index.html" class="logo">FUNCIONAMIENTO</a>
					</header>


				<!-- Nav -->
					<nav id="nav">
						<ul class="links">
							<li><a href="../index.html">Ideas Principales del Proyecto</a></li>
							<li class="active" ><a href ="funcionamiento.html"> Funcionamiento </a></li>
							<li><a href="datasets.html">Datasets y Scripts</a></li>
							<li><a href="sobreNosotros.html">Sobre Nosotros</a></li>
						</ul>
						<ul class="icons">
							<li><a href="https://github.com/Cloud2020Group4/CovidAnalysis" class="icon brands fa-github"><span class="label">GitHub</span></a></li>
						</ul>
                    </nav>
                    

                    
				<!-- Main -->
					<div id="main">

                        <article class="post featured">
                           
                            
                            <div class="table-wrapper">

                                <h3 id="menuP1" onclick="toggleMenuP1()" class="button small fit"><header>¿Cómo Funciona?</header></h3>

                                <div id= "menu-box-P1" style ="display:none">
                                <p>
                                  
                                    <blockquote>
                                       <p>La aplicación consta de una serie de scripts de python, de los cuales, para hacerla funcionar, solo se ejecutará el que se llama main.py.
                                         Este es un script de python estándar que implementa una serie de menús que dan acceso a las distintas funcionalidades con las que cuenta la aplicación,
                                          las cuales están implementadas en el resto de ficheros de la carpeta scripts.</p> 

                                       <p> Una vez hayamos seleccionado qué funcionalidad de la aplicación queremos usar y hayamos introducido los parámetros y opciones necesarios para su
                                         funcionamiento, la aplicación generará automáticamente un ejecutable <code>execute.py</code>, que se guardará también en la carpeta scripts, y también
                                          automáticamente la propia aplicación ejecutará un <code>spark-submit</code> con dicho fichero como parámetro (llamando al sistema con <code>park-submit
                                           execute.py</code>).
                                           Además opcionalmente se nos dará la opción de medir el tiempo que tarda en ejecutarse este script autogenerado de spark, ejecutándose, 
                                           si es que queremos medir el tiempo, la orden <code> spark-submit execute.py.</code></p>
                                       <p>La aplicación generará varios objetos de salida por la ejecución de cada una de sus funcionalidades (es decir, cada vez que automáticamente
                                         genera un script y hace un <code>spark-submit</code> del mismo). Estos objetos se almacenarán en un directorio concreto para cada ejecución dentro de un
                                          directorio general <code>saved_outputs</code>. El directorio concreto de cada ejecución se llama <code>results_yyyy-mm-dd_hh-mm-ss</code>(donde yyyy-mm-dd y hh-mm-ss
                                           se corresponden, respectivamente, con la fecha y hora en la que se lanzó la ejecución de esa funcionalidad concreta). Dentro de ese directorio 
                                           encontraremos dos subdirectorios más, llamados <code>graphs</code> y <code>output</code>, en los que encontramos las gráficas generadas y el dataframe obtenido (en formato csv)
                                            como resultado de la ejecución de cada utilidad de nuestra aplicación.</p> 
                                    </blockquote>


                                </p>
                            </div>
                                    
                                <hr>

                                
                                <h3 id="menuP2" onclick="toggleMenuP2()" class="button small fit"><header>¿Cómo se Ejecuta?</header></h2>
                                <div id="menu-box-P2" style ="display:none">
                                    <p>
                                        <blockquote>
                                        La aplicación está pensada para poder ser ejecutada sin problema y de igual manera en cualquier entorno en el que se pueda hacer uso de Spark,
                                         ya sea en una máquina local o en un cluster de Hadoop. Detallaremos a continuación los pasos a seguir para conseguir tener la aplicación
                                          funcionando en ambos casos.
                                        </blockquote>
                                    </p>
                                <center>Selecciona "MODO LOCAL" o "CLUSTER DE HADOOP EN AWS"</center>
                                <ul class="actions stacked">
                                <li>
                                <h3 id="local" onclicK="toggleLocal()" class="button primary"> Modo LOCAL</h3>
                                <div id="menu-box-local" style="display:none">

                                    <h3>Requisitos</h3>
                                    <p><blockquote>
                                        Para hacer funcionar nuestra aplicación en modo local será necesario que tengamos instalado en nuestra máquina
                                         Java, Python, Spark y Scala. No se requiere ningún requisito especial para las distribuciones anteriores, aunque si se quiere usar la funcionalidad 
                                         de machine learning de nuestra aplicación es necesario contar con Spark-3.0.1.

                                        La aplicación ha sido probada con éxito en una máquina local con Ubuntu 20.04, Python 3.8.5, Java OpenJDK 1.8.0_275, Spark 3.0.1 y Scala 2.11.12, 
                                        aunque no debería dar problemas con otras versiones siempre y  cuando mantengamos para Spark la versión 3.0.1.

                                        Los pasos que se exponen a continuación serían válidos para descargar la aplicación en una máquina con Ubuntu.
                                         Si ya tenemos Spark-3.0.1 funcionando en nuestro sistema podemos saltarnos estos cuatro pasos previos de instalación.
                                    </blockquote></p>
                                    <div id="centrar-lista" style = "text-align: left;">
                                    <ol type="1">
                                        <li><h4>Instalar Java</h4>
                                        <pre>
                                            <code>
$ sudo apt-add-repository ppa:webupd8team/java 
$ sudo apt-get update
$ sudo apt install openjdk-8-jdk
                                            </code>
                                        </pre>

                                        </li>
                                        <li><h4>Instalar Scala</h4>
                                    
                                            <pre>
                                                <code>$ sudo apt-get install scala</code>
                                            </pre>
                                        </li>
                                        <li><h4>Instalar Python</h4>
                                                                                
                                            <pre>
                                                <code>$ sudo apt-get install python</code>
                                            </pre>

                                        </li>
                                        <li><h4>Instalar Spark 3.0.1</h4>
                                                                                
                                            <pre>
                                                <code>$ sudo curl -O https://ftp.cixug.es/apache/spark/spark-3.0.1/spark-3.0.1-bin-hadoop2.7.tgz
$ sudo tar xvf ./spark-3.0.1-bin-hadoop2.7.tgz
$ sudo mkdir /usr/local/spark 
$ sudo cp -r spark-3.0.1-bin-hadoop2.7/* /usr/local/spark</code>
                                            </pre>

                                        </li>
                                 
                                    </ol>
                                    <p>A continuación añadimos spark a la ruta del PATH. En ~/.profile añadimos al final del fichero:</p>
                                   <pre><code>export PATH="$PATH:/usr/local/spark/bin</code></pre> 
                                    <p>Ejecutamos a continuación</p>
                                    <pre><code>$ source ~/.profile</code></pre>
                                    <p>Si estamos en una <b>máquina virtual en AWS</b>debemos añadir en el fichero /etc/hosts el hostname interno y nuestra IP. Si, por ejemplo,
                                         nuestra IP es 172.30.4.210 debemos escribir en el fichero:</p>
                                       <pre><code>
127.0.0.1 localhost
172.30.4.210 ip-172-30-4-210
                                        </code></pre> 

                                    <p>Una vez hecho esto ya tendríamos Spark-3.0.1 funcionando en nuestro sistema.<br>

                                        Para hacer funcionar nuestra aplicación debemos seguir aún algunos pasos adicionales más:<br>
                                        
                                        En primer lugar, para poder utilizar las funcionalidades de generación de gráficas de la 
                                        aplicación es necesario tener instalada la biblioteca de Python Matplotlib. Para instalarla hacemos uso del administrador de paquetes de python, pip.<br>
                                         Si no lo tenemos instalado lo hacemos con:</p>
                                         <pre><code>$ sudo apt-get install python-pip</code></pre>
                                    <p>A continuación instalamos Matplotlib con:</p>
                                        <pre><code>$ pip install matplotlib</code></pre>
                                        <p>Una vez hecho esto es necesario indicar la <b>ruta de nuestra instalación de Python</b> en la variable <code>PYSPARK_PYTHON</code>. Para ello, nuevamente modificamos el fichero <code>~/.profile</code>, añadiendo al final del mismo la línea:</p>
                                        <pre><code>export PYSPARK_PYTHON='/usr/bin/python'</code></pre>
                                        <p>Para actualizar su valor volvemos a ejecutar:</p>
                                        <pre><code>$ source ~/.profile</code></pre>
                                    <p>Con todo esto ya sí que tenemos nuestra máquina local lista para poder ejecutar la aplicación. 
                                        Para descargarla podemos hacerlo directamente desde el enlace de la web (nos descargará un zip únicamente con la parte de Application)
                                         y descomprimiendo el archivo que se descarga o clonando este repositorio con la utilidad <code>git</code>, aunque si hacemos esto último hay que tener en cuenta 
                                         que sólo usaremos
                                         la parte de Application (el resto se corresponde con el código fuente de la web).<br>

                                        Como ya se ha dicho antes, se accede a todas las funcionalidades de la aplicación ejecutando únicamente 
                                        el script <code>main.py</code> como un ejecutable estándar de python. Este script se encuentra en la carpeta scripts dentro 
                                        de la carpeta Application. Por ejemplo, si hemos clonado el repositorio en nuestro directorio local podemos ejecutar la aplicación con:</p>
                                        <pre><code>$ python ~/CovidAnalysis/Application/scripts/main.py</code></pre>
                                        <p>Una vez hecho esto nos aparecerá un menú inicial en el que debemos seleccionar la segunda opción,
                                             ya que estamos ejecutando la aplicación con Spark sobre nuestra máquina local. A continuación debemos 
                                             indicar el paralelismo a nivel de sistema con el que queremos ejecutar nuestra aplicación en Spark, 
                                             esto es, el número de threads que se crearán y que realizarán las distintas tareas de manera distribuida.
                                              Si introducimos un número mayor que 0 se crearán tantos hilos como hayamos especificado <code>(master(local[N])</code>
                                               si hemos introducido que queremos N threads). Si introducimos 0 se nos crearán tantos threads como procesadores 
                                               lógicos tenga nuestra máquina <code>(master(local[*]))</code>.</p>
                                               <pre><code>
**********************
Where are you executing the application?
1.Hadoop Cluster
2.Local mode
**********************
Enter an option: 2
Select desired level of system parallelism, that is, the number of threads that you want to create
Enter 0 if you want to run spark with as many worker threads as logical cores on your machine
Enter a number: 0
</code></pre>
                                    </div>


                                    <a href="#menuP2" class="button icon solid solo fa-arrow-up scrolly"></a> 

                                </div>
                            </li>
                            <li>
                                <h3 id="cluster2" onclicK="toggleCluster()" class="button primary"> CLUSTER DE HADOOP EN AWS</h3>
                                <div id="menu-box-cluster" style="display:none">

                                    <p>
                                        <blockquote>

                                            <div id="code2" style="text-align: left;">
                                                <p>Podemos lanzar nuestra aplicación sobre un cluster de Hadoop, eligiendo Spark como aplicación y teniendo en cuenta que si queremos
                                                 utilizar las funcionalidades 
                                                de machine learning es necesario que el cluster cuente con Spark-3.0.1. Así, lanzamos el cluster en EMR (Elastic Map Reduce)
                                                 de AWS (Amazon Web Services) eligiendo las opciones:</p>
<pre>
<code><li>Release: emr-6.2.0</li>
<li>Applications: Spark: Spark 3.0.1 on Hadoop 3.2.1 YARN with Zeppein 0.9.0-preview1</li>
</code></pre><p>El número de instancias en el cluster y el tipo de estas instancias puede elegirse como se quiera, de hecho probamos 
                                            varias combinaciones para evaluar el rendimiento de nuestra aplicación en todas ellas. Un ejemplo de configuración de
                                             cluster sobre el que funciona la aplicación sería el siguiente:</p>
                                        </div>

                                    </blockquote>
                                        <img src="images/cluster1.jpg" >
                                    <div id="text3" style="text-align: left;">
                                    <p>Una vez lanzado el cluster y cuando estemos conectados con él vía ssh, aún es necesario:</p>
                                    <ol>
                                        <li>Instalar la biblioteca de python matplotlib:</li>
                                        <pre><code>$ sudo pip3 install matplotlib</code></pre>
                                        <li>Exportar la variable PYSPARK_PYTHON:</li>
                                        <pre><code>$ export PYSPARK_PYTHON='/usr/bin/python3'</code></pre>
                                    </ol>
                                    <p>Ahora ya podemos descargar la aplicación. Una manera de hacerlo es clonando este repositorio
                                         (obendríamos todo el respositorio aunque solo vayamos a usar lo contenido en Application. 
                                         Para ello antes debemos instalar la utilidad git. Ejecutamos:</p>
                                         <pre><code>$ sudo yum install git</code></pre>
                                    <p>Para clonar el repositorio hacemos:</p>
                                    <pre><code>$ git clone https://github.com/Cloud2020Group4/CovidAnalysis.git</code></pre>
                                    <p>Ya podemos ejecutar nuestra aplicación, que, como ya se ha dicho, solo es necesario ejecutar el script main.py, 
                                        que se encuentra en la carpeta scripts dentro de la carpeta Application. Si hemos clonado el repositorio con git en ~/,
                                         para ejecutar la aplicación basta con hacer:</p>
                                         <pre><code>$ python3 ~/CovidAnalysis/Application/scripts/main.py</code></pre>
                                         <p>Una vez ejecutada nos aparecerá un primer menú en el que debemos seleccionar la opción 1, pues vamos a lanzar Spark sobre un cluster de Hadoop.

                                            A continuación debemos configurar el paralelismo a nivel de sistema con el que lanzaremos Spark, es decir, dar valor a los flags 
                                            <code>--num-executors</code> y <code>--executor-cores</code> que acompañan a spark-submit.<BR>
                                            
                                            Primero se nos pedirá el número de nodos del cluster que queremos que realicen trabajos (<code>--num-executors</code>), y a continuación, el número de threads en cada trabajador (<code>--executor-cores</code>).
                                             El valor máximo para el primero de los parámetros vendrá dado por el número de instancias que hemos lanzado como "worker nodes" y para el segundo de los parámetros por el número de cores
                                              de cada instancia del cluster (por ejemplo, si hemos elegido instancias de m4.xlarge, podemos establecer un valor máximo de 4 para este segundo parámetro). Para ambos valores podemos
                                               introducir 0 si no queremos darle valor a esos flags y la ejecución se realizará tratando de optimizar los recursos en el cluster.<BR>
                                            
                                            Seguidamente se nos preguntará si queremos subir los datasets al Hadoop File System. La ejecución de Spark, al realizarse sobre distintos nodos, toma sus datos del sistema 
                                            de ficheros de Hadoop, común a todos los nodos, por lo que es necesario que los ficheros se encuentren ahí. Podemos hacer esto manualmente, pero para no tener que complicarnos,
                                             la primera vez que ejecutemos la aplicación indicamos que sí que queremos subir los ficheros al HDFS y ya los tendremos ahí disponibles para todas las ejecuciones de la aplicación
                                              que realicemos en el cluster.</p>

                                              <pre><code>**********************
 Where are you executing the application?
1.Hadoop Cluster
2.Local mode
**********************
Enter an option: 1
Configure the system parallelism when running spark-submit
Select the value for the flag --num-executors (0 if you don't want to set it): 2
Select the value for the flag --executor-cores (0 if you don't want to set it): 4
Do you want to upload the datasets to Hadoop File System? (you must do it the first time you run the application)[y/n]: y
Updating datasets to Hadoop File System...
Updating ended...</code></pre>
 
                                    </div>
                                    
                                    <a href="#cluster2" class="button icon solid solo fa-arrow-up scrolly"></a> 
                                </div>
                            </div>
                        </li>
                    </ul>
                            <hr>
                            <h3 id="menuP5" onclick="toggleMenuP5()" class="button small fit"><header>¿Qué podemos hacer con la aplicación?</header></h2>
                                <div id="menu-box-P5" style ="display:none">
                                <blockquote>
                                    Una vez hayamos seguidos los pasos anteriores y hayamos ejecutado la aplicación
                                     y configurado el paralelismo local, la aplicación nos mostrará el siguiente menú principal:

                                     <pre><code>

**********************
Menu
1.DOWNLOAD THE NEWEST DATASET WITH COVID-19 DATA
2.Covid-19 data
3.Economic data
4.Populational data
5.Health data
6.Machine Learning
7.Exit
**********************
Enter your choice:
</code></pre>

                                <p>La aplicación funciona con una serie de datasets. Uno de ellos (proporcionado por <a href="https://ourworldindata.org/" >Our World in Data</a> ) recoge los datos diarios de Covid-19 por países,
                                    además de varios indicadores económicos, demográficos y de salud útiles para el estudio de la incidencia de la pandemia. Cuando descargas la aplicación
                                    esta incluye una versión de este dataset desactualizada. Si queremos trabajar con los últimos datos disponibles (hasta el día anterior al que estamos) 
                                    debemos ejecutar la opción 1 y ya podremos trabajar con datos lo más actuales posible. Si estamos ejecutando en un cluster de Hadoop, al ejecutar esta
                                    opción se actualiza también el fichero en el HDFS, por lo que no es necesario hacer nada más que seleccionar la opción en el menú.<br>

                                    Las opciones 2-6 dan acceso a las utilidades de la aplicación como tal. En todas ellas podemos ver valores de distintos indicadores de esa categoría,
                                    obtener datos por países, elaborar listas con los mejores o peores países en un determinado aspecto, agregar datos por continentes o comparar resultados
                                    para dos países, todo ello obteniendo los datos en un dataframe que se guardará en formato csv y en muchos de ellos con la opción también de representar
                                    gráficamente esos datos.<br>
                                    
                                    Elaborar una gráfica comparando los casos diarios de covid-19 entre España y Portugal, ver la evolución de muertes por Covid-19 en Italia desde el día que empezó
                                    su confinamiento hasta la finalización del mismo, obtener una lista de los países con más porcentaje anciano de población, comparar las facilidades para lavarse las
                                    manos por continentes, obtener la opinión de la población de un país sobre la efectividad de las vacunas o ver si existe correlación entre el número de casos de Covid
                                    en un país y su GDP con algoritmos de Clustering son algunas de las muchas opciones a las que se tienen acceso con esta aplicación. Simplemente, basta con seleccionar
                                    qué queremos hacer en cada momennto entre las opciones disponibles y obtener los resultados. Aquí podemos ver unos ejemplos:<br><p>
                                          <p><center> <a href="graficas.html"><button class="button primary">GRAFICAS</button></a></center></p> 
                                    
                
                                    
                                   <p> La aplicación muestra total flexibilidad y nos permite conseguir gran variedad de datos y gráficas útiles para un análisis de los mismos, pudiendo parametrizar por países, 
                                    fechas, indicadores...</p>
                                </blockquote>

                                <a href="#menuP5" class="button icon solid solo fa-arrow-up scrolly"></a> 
                            </div>
                                
                        </div>

                            

                    </article>

                        



					</div>


                    				<!-- Copyright -->
					<div id="copyright">
                        <ul><li>CARLA MARTÍNEZ NIETO-MARQUEZ</li><li>DANIEL LEDESMA VENTURA</li><li>JAVIER GUZMÁN MUÑOZ</li><li>GONZALO FERNÁNDEZ MEJÍAS</li><li>PABLO MORER OLMOS</li></ul>
					</div>



		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>


            <script>
                function toggleMenuP1() {
                    var menuBox = document.getElementById('menu-box-P1');    
                        if(menuBox.style.display == "block") { // if is menuBox displayed, hide it
                            menuBox.style.display = "none";
                        }
                        else { // if is menuBox hidden, display it
                            menuBox.style.display = "block";
                        }
                    }

                    function toggleMenuP2() {
                    var menuBox = document.getElementById('menu-box-P2');    
                        if(menuBox.style.display == "block") { // if is menuBox displayed, hide it
                            menuBox.style.display = "none";
                        }
                        else { // if is menuBox hidden, display it
                            menuBox.style.display = "block";
                        }
                    }

                    function toggleMenuP3() {
                    var menuBox = document.getElementById('menu-box-P3');    
                        if(menuBox.style.display == "block") { // if is menuBox displayed, hide it
                            menuBox.style.display = "none";
                        }
                        else { // if is menuBox hidden, display it
                            menuBox.style.display = "block";
                        }
                    }
                    
                    function toggleMenuP5() {
                    var menuBox = document.getElementById('menu-box-P5');    
                        if(menuBox.style.display == "block") { // if is menuBox displayed, hide it
                            menuBox.style.display = "none";
                        }
                        else { // if is menuBox hidden, display it
                            menuBox.style.display = "block";
                        }
                    }


                    function toggleLocal() {
                    var menuBox = document.getElementById('menu-box-local');    
                        if(menuBox.style.display == "block") { // if is menuBox displayed, hide it
                            menuBox.style.display = "none";
                        }
                        else { // if is menuBox hidden, display it
                            menuBox.style.display = "block";
                        }
                    }

                    function toggleCluster() {
                    var menuBox = document.getElementById('menu-box-cluster');    
                        if(menuBox.style.display == "block") { // if is menuBox displayed, hide it
                            menuBox.style.display = "none";
                        }
                        else { // if is menuBox hidden, display it
                            menuBox.style.display = "block";
                        }
                    }
            </script>


	</body>
</html>